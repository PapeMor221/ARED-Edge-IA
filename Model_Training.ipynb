{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Fonction pour analyser une ligne de log\n",
    "def parse_log_line(line):\n",
    "    # Définir plusieurs patterns pour différents formats de logs\n",
    "    patterns = [\n",
    "        re.compile(r'(?P<date>\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2}) (?P<host>\\S+) (?P<service>\\S+)\\[(?P<pid>\\d+)\\]: (?P<message>.+)'),\n",
    "        re.compile(r'(?P<date>\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+\\d{4}) (?P<host>\\S+) (?P<service>\\S+)\\[(?P<pid>\\d+)\\]: (?P<message>.+)'),\n",
    "        re.compile(r'(?P<date>\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2}) (?P<host>\\S+) (?P<service>\\S+): (?P<message>.+)'),\n",
    "        re.compile(r'\\[(?P<date>[\\d\\.]+)\\] (?P<message>.+)'),\n",
    "        re.compile(r'(?P<host>\\S+) (?P<service>\\S+)\\[(?P<pid>\\d+)\\]: (?P<message>.+)'),\n",
    "        #re.compile(r'(?P<message>.+)'),\n",
    "        re.compile(r'(?P<date>\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+\\d{4}) (?P<host>\\S+) (?P<service>\\S+): (?P<message>.+)'),\n",
    "        re.compile(r'(?P<date>\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d+Z) level=(?P<level>\\w+) msg=\"(?P<message>.+)\"'),\n",
    "        re.compile(r'(?P<host>\\S+) (?P<service>\\S+)\\[(?P<pid>\\d+)\\]: (?P<message>.+)')\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            log_dict = match.groupdict()\n",
    "            \n",
    "            # Nettoyage spécifique pour certains services\n",
    "            if 'service' in log_dict:\n",
    "                if log_dict['service'].startswith('balena-engine-daemon'):\n",
    "                    message = log_dict['message']\n",
    "                    message_cleaned = re.sub(r'time=\"[^\"]+\" ', '', message)  # Suppression des timestamps\n",
    "                    message_cleaned = re.sub(r'namespace=[^ ]+ ', '', message_cleaned)  # Suppression des namespaces\n",
    "                    log_dict['message'] = message_cleaned\n",
    "                \n",
    "                elif log_dict['service'].startswith('python3'):\n",
    "                    message = log_dict['message']\n",
    "                    message_cleaned = re.sub(r'\\[pid \\d+\\] ', '', message)  # Suppression des PIDs\n",
    "                    log_dict['message'] = message_cleaned\n",
    "\n",
    "                elif log_dict['service'].startswith('chilli'):\n",
    "                    message = log_dict['message']\n",
    "                    message_cleaned = re.sub(r'chilli\\[\\d+\\]: ', '', message)  # Suppression des PIDs et du nom du service\n",
    "                    log_dict['message'] = message_cleaned\n",
    "            \n",
    "            return log_dict\n",
    "    \n",
    "    return None\n",
    "\n",
    "def save_logs_to_csv(logs, output_file):\n",
    "    # Convertir la liste de dictionnaires en DataFrame\n",
    "    df = pd.DataFrame(logs)\n",
    "    # Sauvegarder le DataFrame dans un fichier CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "# Fonction pour lire tous les fichiers de logs dans un dossier et les étiqueter\n",
    "def process_log_files(directory, label):\n",
    "    logs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):  # Filtrer les fichiers de log\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                log_lines = file.readlines()\n",
    "            parsed_logs = [parse_log_line(line) for line in log_lines if parse_log_line(line) is not None]\n",
    "            for log in parsed_logs:\n",
    "                log['label'] = label\n",
    "            logs.extend(parsed_logs)\n",
    "    return logs\n",
    "\n",
    "# Exemple d'utilisation\n",
    "log_directory_1 = 'label1'  # Dossier contenant les fichiers de logs à étiqueter avec 1\n",
    "log_directory_0 = 'label0'  # Dossier contenant les fichiers de logs à étiqueter avec 0\n",
    "\n",
    "logs_label_1 = process_log_files(log_directory_1, 1)\n",
    "logs_label_0 = process_log_files(log_directory_0, 0)\n",
    "\n",
    "all_logs = logs_label_1 + logs_label_0\n",
    "save_logs_to_csv(all_logs, 'all_logs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('all_logs.csv')\n",
    "\n",
    "# Préparation des données\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['message']).toarray()\n",
    "y = df['label']\n",
    "\n",
    "# Appliquer SMOTE pour suréchantillonner les données\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Convertir les tableaux numpy en DataFrames\n",
    "X_res_df = pd.DataFrame(X_res, columns=vectorizer.get_feature_names_out())\n",
    "y_res_df = pd.DataFrame(y_res, columns=['label'])\n",
    "\n",
    "# Recombiner les données suréchantillonnées en DataFrame\n",
    "df_res = pd.concat([X_res_df, y_res_df], axis=1)\n",
    "\n",
    "# Sauvegarder les données suréchantillonnées dans un nouveau fichier CSV\n",
    "df_res.to_csv('logs_resampled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/papamor/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9158 - loss: 0.3052 - val_accuracy: 0.9989 - val_loss: 0.0037\n",
      "Epoch 2/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0117 - val_accuracy: 0.9995 - val_loss: 0.0029\n",
      "Epoch 3/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0059 - val_accuracy: 0.9995 - val_loss: 0.0033\n",
      "Epoch 4/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0058 - val_accuracy: 0.9995 - val_loss: 0.0032\n",
      "Epoch 5/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0072 - val_accuracy: 0.9989 - val_loss: 0.0038\n",
      "Epoch 6/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0055 - val_accuracy: 0.9995 - val_loss: 0.0034\n",
      "Epoch 7/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 0.9995 - val_loss: 0.0035\n",
      "Epoch 8/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0057 - val_accuracy: 0.9995 - val_loss: 0.0033\n",
      "Epoch 9/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0064 - val_accuracy: 0.9995 - val_loss: 0.0038\n",
      "Epoch 10/10\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0061 - val_accuracy: 0.9995 - val_loss: 0.0036\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - accuracy: 0.9999 - loss: 0.0014   \n",
      "Loss: 0.005832513328641653\n",
      "Accuracy: 0.9990290999412537\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Charger les données suréchantillonnées\n",
    "df_res = pd.read_csv('logs_resampled.csv')\n",
    "\n",
    "# Préparation des données\n",
    "X = df_res.drop('label', axis=1).values\n",
    "y = df_res['label'].values\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construire le modèle de réseau de neurones\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       525\n",
      "           1       1.00      1.00      1.00       505\n",
      "\n",
      "    accuracy                           1.00      1030\n",
      "   macro avg       1.00      1.00      1.00      1030\n",
      "weighted avg       1.00      1.00      1.00      1030\n",
      "\n",
      "[[524   1]\n",
      " [  0 505]]\n"
     ]
    }
   ],
   "source": [
    "# Faire des prédictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['log_vectorizer.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save('log_model.h5')\n",
    "\n",
    "# Sauvegarder le vectorizer\n",
    "joblib.dump(vectorizer, 'log_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
